---
title: "Group 7 Project 2: Models"
author: "Andy Christian, Yaxin (Janet) Zhuang, Adhithya Kiran, Yasaswi Kasaraneni"
date: "5/01/2022"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F)
options(scientific=T, digits = 3)

#install packages
#install.packages("DT")
#install.packages("tidyverse")
#install.packages("ezids")
#install.packages("knitr")
#install.packages("kableExtra")
#install.packages("xtable")
#install.packages('Hmisc')
#install.packages('MLmetrics')
#install.packages("DescTools")

#load packages
library(DT)
library(ezids)
library(knitr)
library(kableExtra)
library(xtable)
library(Hmisc)
library(regclass)
library(pROC)
library(pscl)
library(glmnet)
library(MLmetrics)
library(DescTools)
library(MASS)
library(car)
library(e1071)
library(caret)
library(cowplot)
library(caTools)
library(pROC)
library(ggcorrplot)
library(tidyverse)

```


```{r}
#Load data sets

#Main data set
churn_df <- data.frame(read_csv('Data Files/Telco_customer_churn.csv', col_types=cols()))

#additional data sets
churn_status_df <- data.frame(read_csv('Data Files/Telco_customer_churn_status.csv', col_types = cols()))
churn_services_df <- data.frame(read_csv('Data Files/Telco_customer_churn_services.csv', col_types = cols()))

##################################################

#Join and filter

#Filter columns to join from status_df
status_join <- churn_status_df %>%
  select(c(Customer.ID, Satisfaction.Score, Churn.Category))
#status_join

#Filer columns to join from services)df
service_join <- churn_services_df %>%
  select(c(Customer.ID, Avg.Monthly.Long.Distance.Charges, Avg.Monthly.GB.Download, Unlimited.Data, Total.Extra.Data.Charges))
#service_join

#Join status with churn
joined_churn_df <- merge(churn_df, status_join, by.x="CustomerID", by.y="Customer.ID")
#head(joined_churn_df)

#Join service with churn
joined_churn_df <- merge(joined_churn_df, service_join, by.x="CustomerID", by.y="Customer.ID")
#head(joined_churn_df)

##################################################

#drop unwanted columns from final df
filtered_joined_churn_df <- joined_churn_df %>%
  select(-c(1:10, 16, 20, 22:23, 28, 30:32))

##################################################

#Convert categorical to factor
cols <- c(1:3, 5:12, 14:17, 20)
filtered_joined_churn_df[cols] <- lapply(filtered_joined_churn_df[cols], as.factor)

##################################################

#Separate into churn and non-churn groupings for later comparison
only_churn <- joined_churn_df %>%
  filter(Churn.Value==1)

no_churn <- joined_churn_df %>%
  filter(Churn.Value==0)

##################################################

#Functions

#Create updated xkable_summary function to delete unwanted text
xkablesum_updated <- function (df, title = "Table: Statistics summary.", digits = 4,
          pos = "left", bso = "striped")
{
    s = summary(df)

    # Including RE NA's strip
    # Needed to add a dim check because including NA strip when no NA row made a new 7th row with text Min.:
    if (dim(s)[1] == 6) {
      strip_vector = c("Min.\\s*:\\s*", "1st Qu.\\s*:\\s*", "Median\\s*:\\s*", "Mean\\s*:\\s*",
                       "3rd Qu.\\s*:\\s*", "Max.\\s*:\\s*")
    }
    # NA's strip RE added here
    else if (dim(s)[1] == 7) {
      strip_vector = c("Min.\\s*:\\s*", "1st Qu.\\s*:\\s*", "Median\\s*:\\s*", "Mean\\s*:\\s*",
                       "3rd Qu.\\s*:\\s*", "Max.\\s*:\\s*", "NA's\\s*:\\s*")
    }

    # Made s = apply() -- without, didn't apply changes to table
    s <- apply(s, 2, function(x) stringr::str_remove_all(x, strip_vector))

    # Made s = apply()
    s <- apply(s, 2, function(x) stringr::str_trim(x, "right"))
    colnames(s) <- stringr::str_trim(colnames(s))

    if (dim(s)[1] == 6) {
        rownames(s) <- c("Min", "Q1", "Median",
                         "Mean", "Q3", "Max")
    }
    else if (dim(s)[1] == 7) {
        rownames(s) <- c("Min", "Q1", "Median",
                         "Mean", "Q3", "Max", "NA")
    }
    xkabledply(s, title = title, digits = digits, pos = pos,
               bso = bso)
}

#Better looking version of 2-sample t-test results than what the object itself displays
ttest2sample_info <- function(test) {
  if (test[["p.value"]] <= 0.05) {
  result = 'Reject the Null Hypothesis'
  } else {
    result = 'Do not reject the Null Hypothesis'
  }

  cat(c('\t', test$method, '\n\n',
      'Data:                       ', '|   ', test$data.name, '\n',
      'Null Hypothesis:            ', '|   true difference in means = ', test$null.value[1], '\n',
      'Alternative Hypothesis:     ', '|   true difference in means != ', test$null.value[1], '\n',
      'Confidence Level:           ', '|   ', attributes(test$conf.int)$conf.level, '\n',
      'Confidence Interval:        ', '|   [', round(test$conf.int[1], 2), ', ', round(test$conf.int[2], 2)), ']\n',
      'Sample Estimates of Mean:   ', '|   [X = ', test$estimate[1], ', Y = ', test$estimate[2], ']\n',
      'Test Values:                ', '|   [t = ', test$statistic,
                                   ', df = ', test$parameter[1],
                                   ', p-value = ', test[["p.value"]], ']\n',
      'Result:                     ', '|   ', result, sep='')
}

```

Ready to continue...

<br>

---

# 1. Quick Review of Data Set

This data set is a fictional data set provided by IBM. It includes the demographic information on the customers of a telecommunications company, data on the services each customer used, charges paid (etc.), as well as whether the customer “churned” or not – meaning, whether the customer left the company in the past month. It also includes data on the reason(s) each customer who did leave gave for leaving.

<br>

#### Data Set Structure

```{r, comments=NA}
str(filtered_joined_churn_df)
```

<br>

From the graph below, we see that around 27% of customer left the platform within the last month.


```{r}
options(repr.plot.width = 6, repr.plot.height = 4)
filtered_joined_churn_df %>%
group_by(Churn.Label) %>%
summarise(Count = n())%>%
mutate(percent = prop.table(Count)*100)%>%
ggplot(aes(reorder(Churn.Label, -percent), percent), fill = Churn)+
geom_col(fill = c("#FC4E07", "#E7B800"))+
geom_text(aes(label = sprintf("%.2f%%", percent)), hjust = 0.01,vjust = -0.5, size =3)+
theme_bw()+  
xlab("Churn") +
ylab("Percent")+
ggtitle("Churn Percent")

```

<br>

The graph below details the reasons customers gave for leaving.
Although some customers may have selected the same reason for leaving, they may have categorized how they associated that reason differently. The churn category is used as a fill to provide further detail in understanding the customer perspective.
We will use these reasons to help us formulate our questions, select appropriate variables to investigate, and contextualize the results:

```{r}
joined_churn_df %>%
  filter(!is.na(Churn.Category)) %>%
  ggplot() +
  geom_bar(mapping=aes(y=Churn.Reason, fill=Churn.Category), show.legend=TRUE) +
  labs(title='Reasons for Leaving by Answer', y=('Reason'), x=('Count')) +
  theme(plot.title = element_text(hjust = 0.5))

```


# 2. Quick Review of EDA and SMART QUESTIONS

Answering SMART Question 1

1. Which of the variables in the data set show a statistical difference between the churn and non-churn groupings?

To answer this question, we divided the data into churn and non-churn groups, then visually explored how the data set’s variables differed when divided into these groups. We then tested the differences using 2-sample t-tests, goodness of fit chi squared tests, and chi squared tests of independence. From this we determined that the following 15 variables showed a statistical difference between the churn and non-churn groups:

* `Tenure`
* `Avg. Monthly Charges`
* `Avg. Monthly GB Download`
* `Senior Citizen`
* `Partner`
* `Dependents`
* `Internet Service`
* `Online Security`
* `Online Backup`
* `Tech Support`
* `Contract Type`
* `Paperless Billing`
* `Payment Method`
* `Satisfaction Score`
* `Unlimited Data`

Smart Questions to answer with modeling:

* Customers with what behaviors and conditions would likely leave the platform?
* What services are important to deliver to a customer to keep them with the company?


# 3. Models

## 3.1 Logistic Model

<br>

### 3.1.1 Satisfaction Logistic Model 

<br>

First, let's explore how `Satisfcation Score` is best understood and used. Is it as a factor or conceptual framework?


```{r results='markup'}
sat_logit_model <- glm(Churn.Label ~ Satisfaction.Score, data=filtered_joined_churn_df, family="binomial")
#summary(sat_logit_model)
xkabledply(sat_logit_model)

```

<br>

Exponentiate both sides of equation to get growth and decay factors...

```{r results='markup'}
#exponentiate both sides of the equation
expcoeff <- exp(coef(sat_logit_model))
xkabledply(as.table(expcoeff), title='Growth and Decay Factors After Exponentiation') 
```

<br>

**Exponentiated Equation Satisfaction**

<br>

$\frac{p}{q} = `r format(expcoeff[1], digits=6)` * (`r format(expcoeff[2], digits=6)`)^{sat2} * (`r format(expcoeff[3], digits=6)`)^{sat3} * (`r format(expcoeff[4], digits=6)`)^{sat4} * (`r format(expcoeff[5], digits=6)`)^{sat5}$

<br>

**Predictions: Satisfaction Model**

```{r results='markup', comment=NA}
#Create new DF of 5 rows, one for each level of satisfaction in order to make predictions
pred_df <- data.frame(Satisfaction.Score=as.factor(c(1,2,3,4,5)))
#Determine likelihood of Churn using Satisfaction Model
pred_response <- predict(sat_logit_model, newdata = pred_df, type = "response")

#Create prediction response df with results of model calculations
pred_response_df <- data.frame(pred_response)
#Clean up formatting of DF
pred_response_df <- pred_response_df %>%
  format(scientific=FALSE) %>%
  cbind(newColName = rownames(pred_response_df), pred_response_df) %>%
  select(c(2,3)) %>%
  mutate(
    Prediction=ifelse(pred_response > 0.5, "Churn", "Non-Churn")
  )
colnames(pred_response_df) <- c("Satisfaction Score", "Churn Likelihood", "Prediction")

#display DF as table
xkabledply(pred_response_df, title="Churn Predictions Using Only Satisfaction Score and a Cut Off of 0.5")
```

<br>

#### **Satisfaction Model Tests**

<br>

**Coefficient P-values: Satisfaction Model**

All p-values are high

---

**Confusion Matrix: Satisfaction Model**

```{r results='markup'}
#Create and display the confusion matrix
confusion_matrix_sat <- xkabledply(confusion_matrix(sat_logit_model), title = "Confusion Matrix for Satisfcation Logistic Model" )
confusion_matrix_sat

```

```{r}
#Turn from kable into data frame (was having trouble accessing elements as a kable)
confusion_matrix_sat_df <- data.frame(confusion_matrix(sat_logit_model))
#confusion_matrix_sat_df

accuracy <- (confusion_matrix_sat_df[1,1] + confusion_matrix_sat_df[2,2])/confusion_matrix_sat_df[3,3]
precision <- confusion_matrix_sat_df[2,2]/(confusion_matrix_sat_df[2,2] + confusion_matrix_sat_df[1,2])
recall_rate <- confusion_matrix_sat_df[2,2]/(confusion_matrix_sat_df[2,2] + confusion_matrix_sat_df[2,1])
specificity <- confusion_matrix_sat_df[1,1]/(confusion_matrix_sat_df[1,1] + confusion_matrix_sat_df[1,2])
f1_score <- 2*(precision)*(recall_rate)/(precision + recall_rate)

# 1. **Accuracy:** *(TP + TN)/Total*
# * `r (confusion_matrix_test_df[1,1] + confusion_matrix_test_df[2,2])/confusion_matrix_test_df[3,3]`
# * The model correctly predicts survived or died `r round((confusion_matrix_test_df[1,1] + confusion_matrix_test_df[2,2])/confusion_matrix_test_df[3,3], digits=3)*100`% of the time.
# 2. **Precision:** *TP/(TP + FP)*
# * `r confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[1,2])`
# * Of the values that are predicted true by the model, `r round(confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[1,2]), digits=3)*100`% of them actually are true.
# 3. **Recall Rate:** *TP/(TP + FN)*
# * `r confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[2,1])`
# * Of the values that actually are true, the model correctly predicts `r round(confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[2,1]), digits=3)*100`% of them as true.
# 4. **Specificity:** *TN/(TN + FP)*
# * `r confusion_matrix_test_df[1,1]/(confusion_matrix_test_df[1,1] + confusion_matrix_test_df[1,2])`
# * Of the values that are actually false, the model correctly predicts `r round(confusion_matrix_test_df[1,1]/(confusion_matrix_test_df[1,1] + confusion_matrix_test_df[1,2]), digits=3)*100`% of them as false.
# 5. **F1 Score:** *2(Precision)(Recall)/(Precision + Recall)*
# * `r 2*(0.753)*(0.714)/(0.753 + 0.714)`
# * The harmonic mean of Precision and Recall, a more balanced view of how good the model is.

```


1. **Accuracy:** *(TP + TN)/Total*
* `r accuracy`
* The model correctly predicts churn or non-churn `r round(accuracy, digits=3)*100`% of the time.
2. **Precision:** *TP/(TP + FP)*
* `r precision`
* Of the values that are predicted true by the model, `r round(precision, digits=3)*100`% of them actually are true.
3. **Recall Rate:** *TP/(TP + FN)*
* `r recall_rate`
* Of the values that actually are true, the model correctly predicts `r round(recall_rate, digits=3)*100`% of them as true.
4. **Specificity:** *TN/(TN + FP)*
* `r specificity`
* Of the values that are actually false, the model correctly predicts, `r round(specificity, digits=3)*100`% of them as false.
5. **F1 Score:** *2(Precision)(Recall)/(Precision + Recall)*
* `r f1_score`
* The harmonic mean of Precision and Recall, a more balanced view of how good the model is.

---

**ROC-AUC: Satisfaction Model**

Area under the curve, testing the true positive rate (or recall) against the false positive rate (or specificity).

```{r}
#Get the probability of survival for each passenger from the model
prob_test=predict(sat_logit_model, type = "response" )
#Create new column in df of predicted probability of survival 
filtered_joined_churn_df$prob=prob_test
#Determine the ratio of true-positive-rate/false-positive rate as a curve between 0.5 and 1
roc_data <- roc(Churn.Label~prob, data=filtered_joined_churn_df)
#Plot as graph
plot(roc_data)
#Value of area under the curve, prefer 0.8 or higher.
#auc(roc_data2) # area-under-curve 
# unloadPkg("pROC")
```

*Comments*

* This model produces a score of `r auc(roc_data)`
* This is above the threshold of 0.8 for a good model fit
* a value of 1 would be a perfect model

---

**McFadden: Satisfaction Model**

```{r, comment=NA}
#calculate McFadden statistics, the pseudo r^2
sat_pr2 = pR2(sat_logit_model)
#sat_pr2[4]

```

The pseudo $r^2$ value for the Satisfaction model, using the McFadden method of calculation, is `r sat_pr2[4]`. This means that `r round(sat_pr2[4], digits=3)*100`% of the variance seen in the data is accounted for by this model.

---

**AIC/BIC and Deviance: Satisfaction Model**

* The AIC of the Satisfaction model is: `r sat_logit_model$aic` 
* The Residual Deviance of the Satisfaction model is: `r sat_logit_model$deviance`

---

#### **Satisfaction Model Takeaways**

* A customer's satisfaction score almost perfectly predicts Churn and Non-Churn
* All 1-2 satisfaction score customers were Churn
* All 4-5 satisfaction score customer were Non-Churn
* Most 3 satisfaction score customers were Non-Churn
* However, what this really means is that satisfaction is basically a stand in for Churn/Non-Churn. It does not provide new information useful for determining what factors are associated with the decision to Churn.
* Satisfaction is better used as the conceptual framework for understanding the customer's motivation in making the decision to Churn or not.

---

<br>

### 3.1.2 Full Logistic Model

<br>

Next, let's look at a baseline model that uses all potential explanatory factors in the data set, with the exception of Average Monthly Long Distance Charges and Total Extra Data charges, which were shown in the EDA to not have statistical significance between their different levels.  


```{r results='markup'}

#Everything model - those factors dropped in EDA

#Senior.Citizen + Partner + Dependents + Tenure.Months + Phone.Service + Internet.Service + Online.Security + Online.Backup + Tech.Support + Contract + Paperless.Billing + Payment.Method + Monthly.Charges + Avg.Monthly.GB.Download + Unlimited.Data

#NOT USING:
#Average Monthly Long Distance
#Total Extra Data Charges
#Satisfaction Score

all_logit_model <- glm(Churn.Label ~ Senior.Citizen + Partner + Dependents + Tenure.Months + Phone.Service + Internet.Service + Online.Security + Online.Backup + Tech.Support + Contract + Paperless.Billing + Payment.Method + Monthly.Charges + Avg.Monthly.GB.Download + Unlimited.Data, data=filtered_joined_churn_df, family="binomial")
#summary(all_logit_model)
xkabledply(all_logit_model, title="Churn.Label ~ All Factors")

```

<br>

Exponentiate both sides of equation to get growth and decay factors...

```{r results='markup'}
#exponentiate both sides of the equation
expcoeff <- exp(coef(all_logit_model))
xkabledply(as.table(expcoeff), title='Growth and Decay Factors After Exponentiation') 
```

<br>

**Exponentiated Equation**

<br>

$\frac{p}{q} = `r format(expcoeff[1], digits=6)` * (`r format(expcoeff[2], digits=6)`)^{senior} * (`r format(expcoeff[3], digits=6)`)^{partner} * (`r format(expcoeff[4], digits=6)`)^{dependents} * (`r format(expcoeff[5], digits=6)`)^{tenure} * (`r format(expcoeff[6], digits=6)`)^{phone}$  

$*  (`r format(expcoeff[7], digits=6)`)^{fiberOptic} * (`r format(expcoeff[8], digits=6)`)^{noInt} * (`r format(expcoeff[10], digits=6)`)^{oSecurity} * (`r format(expcoeff[12], digits=6)`)^{oBackup} * (`r format(expcoeff[14], digits=6)`)^{techSup} * (`r format(expcoeff[15], digits=6)`)^{cont1yr}$  

$*  (`r format(expcoeff[16], digits=6)`)^{cont2yr} * (`r format(expcoeff[17], digits=6)`)^{paperless} * (`r format(expcoeff[18], digits=6)`)^{pmAutoCC} * (`r format(expcoeff[19], digits=6)`)^{pmEcheck} * (`r format(expcoeff[20], digits=6)`)^{mailCheck}$

$*  (`r format(expcoeff[21], digits=6)`)^{monthlyCharge} * (`r format(expcoeff[22], digits=6)`)^{monthlyGB} * (`r format(expcoeff[23], digits=6)`)^{unlimited}$

<br>

---

#### **Full Model Tests**

<br>

**Coefficient P-values: Full Model**

High p-values for the following factors:

* Senior
* Fiber optic internet
* No internet service
* PM auto credit card
* PM mailed check
* Monthly GB
* Unlimited data

---

**Confusion Matrix: Full Model**

```{r results='markup'}
#Create and display the confusion matrix
confusion_matrix_all <- xkabledply(confusion_matrix(all_logit_model), title = "Confusion Matrix for Full Logistic Model, Cut Off of 0.5" )
confusion_matrix_all

```

```{r}
#Turn from kable into data frame (was having trouble accessing elements as a kable)
confusion_matrix_all_df <- data.frame(confusion_matrix(all_logit_model))
#confusion_matrix_all_df

accuracy <- (confusion_matrix_all_df[1,1] + confusion_matrix_all_df[2,2])/confusion_matrix_all_df[3,3]
precision <- confusion_matrix_all_df[2,2]/(confusion_matrix_all_df[2,2] + confusion_matrix_all_df[1,2])
recall_rate <- confusion_matrix_all_df[2,2]/(confusion_matrix_all_df[2,2] + confusion_matrix_all_df[2,1])
specificity <- confusion_matrix_all_df[1,1]/(confusion_matrix_all_df[1,1] + confusion_matrix_all_df[1,2])
f1_score <- 2*(precision)*(recall_rate)/(precision + recall_rate)

# 1. **Accuracy:** *(TP + TN)/Total*
# * `r (confusion_matrix_test_df[1,1] + confusion_matrix_test_df[2,2])/confusion_matrix_test_df[3,3]`
# * The model correctly predicts survived or died `r round((confusion_matrix_test_df[1,1] + confusion_matrix_test_df[2,2])/confusion_matrix_test_df[3,3], digits=3)*100`% of the time.
# 2. **Precision:** *TP/(TP + FP)*
# * `r confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[1,2])`
# * Of the values that are predicted true by the model, `r round(confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[1,2]), digits=3)*100`% of them actually are true.
# 3. **Recall Rate:** *TP/(TP + FN)*
# * `r confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[2,1])`
# * Of the values that actually are true, the model correctly predicts `r round(confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[2,1]), digits=3)*100`% of them as true.
# 4. **Specificity:** *TN/(TN + FP)*
# * `r confusion_matrix_test_df[1,1]/(confusion_matrix_test_df[1,1] + confusion_matrix_test_df[1,2])`
# * Of the values that are actually false, the model correctly predicts `r round(confusion_matrix_test_df[1,1]/(confusion_matrix_test_df[1,1] + confusion_matrix_test_df[1,2]), digits=3)*100`% of them as false.
# 5. **F1 Score:** *2(Precision)(Recall)/(Precision + Recall)*
# * `r 2*(0.753)*(0.714)/(0.753 + 0.714)`
# * The harmonic mean of Precision and Recall, a more balanced view of how good the model is.

```


1. **Accuracy:** *(TP + TN)/Total*
* `r accuracy`
* The model correctly predicts churn or non-churn `r round(accuracy, digits=3)*100`% of the time.
2. **Precision:** *TP/(TP + FP)*
* `r precision`
* Of the values that are predicted true by the model, `r round(precision, digits=3)*100`% of them actually are true.
3. **Recall Rate:** *TP/(TP + FN)*
* `r recall_rate`
* Of the values that actually are true, the model correctly predicts `r round(recall_rate, digits=3)*100`% of them as true.
4. **Specificity:** *TN/(TN + FP)*
* `r specificity`
* Of the values that are actually false, the model correctly predicts, `r round(specificity, digits=3)*100`% of them as false.
5. **F1 Score:** *2(Precision)(Recall)/(Precision + Recall)*
* `r f1_score`
* The harmonic mean of Precision and Recall, a more balanced view of how good the model is.

---

**ROC-AUC: Full Model**

Area under the curve, testing the true positive rate (or recall) against the false positive rate (or specificity).

```{r}
#Get the probability of survival for each passenger from the model
prob_test=predict(all_logit_model, type = "response" )
#Create new column in df of predicted probability of survival 
filtered_joined_churn_df$prob=prob_test
#Determine the ratio of true-positive-rate/false-positive rate as a curve between 0.5 and 1
roc_data <- roc(Churn.Label~prob, data=filtered_joined_churn_df)
#Plot as graph
plot(roc_data)
#Value of area under the curve, prefer 0.8 or higher.
#auc(roc_data2) # area-under-curve 
# unloadPkg("pROC")
```

*Comments*

* This model produces a score of `r auc(roc_data)`
* This is above the threshold of 0.8 for a good model fit

---

**McFadden: Full Model**

```{r, comment=NA}
#calculate McFadden statistics, the pseudo r^2
all_pr2 = pR2(all_logit_model)
#all_pr2[4]

```

The pseudo $r^2$ value for the full model, using the McFadden method of calculation, is `r all_pr2[4]`. This means that `r round(all_pr2[4], digits=3)*100`% of the variance seen in the data is accounted for by this model.

---

**AIC/BIC and Deviance: Full Model**

* The AIC of the full model is: `r all_logit_model$aic` 
* The Residual Deviance of the full model is: `r all_logit_model$deviance` 

---

#### **Full Model Takeaways**

* The high p-values for the coefficients of a number of factors indicates that further feature selection is needed.
* The relatively low precision and recall rate suggests that a cut off score other than 0.5 needs to be used.
* However, an AUC of 0.858 indicates that we are on the right track.

---

### 3.1.3 Final Logistic Model

<br>

To build a better model for the final version, we will drop those variables that had high p-values in the full model. After some experiementation based on dropping high p-value factors, the final model includes the following factors:

* `Dependents`
* `Phone Service`
* `Contract`
* `Monthly Charges`
* `Monthly GB`


```{r results='markup'}
# #Attempt Lasso regression for numeric variables -- encountered code errors that looked too involved to figure out
# full_mod_sel <- filtered_joined_churn_df %>%
#   select(c(4,13,14,19)) #18?
# 
# #x <- as.matrix(full_mod_sel[,-3]) # all X vars
# x <- full_mod_sel[, -3]
# y <- as.double(as.matrix(ifelse(full_mod_sel[, 4]=='Yes', 1, 0))) # Only Churn.Label
# 
# # Fit the LASSO model (Lasso: Alpha = 1)
# set.seed(100)
# cv.lasso <- cv.glmnet(x, y, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')
# 
# # Results
# plot(cv.lasso)

#Factors
#Senior.Citizen + Partner + Dependents + Tenure.Months + Phone.Service + Internet.Service + Online.Security + Online.Backup + Tech.Support + Contract + Paperless.Billing + Payment.Method + Monthly.Charges + Avg.Monthly.GB.Download + Unlimited.Data

#Drop senior, type of internet, type of payment, Monthly GB, unlimited (online security, online backup, tech support, unlimited data)
#+ Tenure.Months, partner

#Build model
final_logit_model <- glm(Churn.Label ~ Dependents + Phone.Service + Contract + Monthly.Charges + Avg.Monthly.GB.Download, data=filtered_joined_churn_df, family="binomial")
#summary(final_logit_model)
xkabledply(final_logit_model, title="Churn.Label ~ All Factors")

```

<br>

Exponentiate both sides of equation to get growth and decay factors...

```{r results='markup'}
#exponentiate both sides of the equation
expcoeff <- exp(coef(final_logit_model))
xkabledply(as.table(expcoeff), title='Growth and Decay Factors After Exponentiation') 
```

**Exponentiated Equation**

<br>

$\frac{p}{q} = `r format(expcoeff[1], digits=6)` * (`r format(expcoeff[2], digits=6)`)^{dependents} * (`r format(expcoeff[3], digits=6)`)^{phone} * (`r format(expcoeff[4], digits=6)`)^{cont1yr}$

$* (`r format(expcoeff[5], digits=6)`)^{cont2yr} * (`r format(expcoeff[6], digits=6)`)^{monthlyCharge} * (`r format(expcoeff[7], digits=6)`)^{monthlyGB}$

<br>

#### **Final Model Tests**

<br>

**Coefficient P-values: Final Model**

All p-values low.

---

**Confusion Matrix: Final Model, 0.5 Cut Off**

```{r results='markup'}
#Create and display the confusion matrix
confusion_matrix_final <- xkabledply(confusion_matrix(final_logit_model), title = "Confusion Matrix for Final Logistic Model With a Cut Off Value of 0.5" )
confusion_matrix_final

```

```{r}
#Turn from kable into data frame (was having trouble accessing elements as a kable)
confusion_matrix_final_df <- data.frame(confusion_matrix(final_logit_model))
#confusion_matrix_final_df

accuracy <- (confusion_matrix_final_df[1,1] + confusion_matrix_final_df[2,2])/confusion_matrix_final_df[3,3]
precision <- confusion_matrix_final_df[2,2]/(confusion_matrix_final_df[2,2] + confusion_matrix_final_df[1,2])
recall_rate <- confusion_matrix_final_df[2,2]/(confusion_matrix_final_df[2,2] + confusion_matrix_final_df[2,1])
specificity <- confusion_matrix_final_df[1,1]/(confusion_matrix_final_df[1,1] + confusion_matrix_final_df[1,2])
f1_score <- 2*(precision)*(recall_rate)/(precision + recall_rate)

# 1. **Accuracy:** *(TP + TN)/Total*
# * `r (confusion_matrix_test_df[1,1] + confusion_matrix_test_df[2,2])/confusion_matrix_test_df[3,3]`
# * The model correctly predicts survived or died `r round((confusion_matrix_test_df[1,1] + confusion_matrix_test_df[2,2])/confusion_matrix_test_df[3,3], digits=3)*100`% of the time.
# 2. **Precision:** *TP/(TP + FP)*
# * `r confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[1,2])`
# * Of the values that are predicted true by the model, `r round(confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[1,2]), digits=3)*100`% of them actually are true.
# 3. **Recall Rate:** *TP/(TP + FN)*
# * `r confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[2,1])`
# * Of the values that actually are true, the model correctly predicts `r round(confusion_matrix_test_df[2,2]/(confusion_matrix_test_df[2,2] + confusion_matrix_test_df[2,1]), digits=3)*100`% of them as true.
# 4. **Specificity:** *TN/(TN + FP)*
# * `r confusion_matrix_test_df[1,1]/(confusion_matrix_test_df[1,1] + confusion_matrix_test_df[1,2])`
# * Of the values that are actually false, the model correctly predicts `r round(confusion_matrix_test_df[1,1]/(confusion_matrix_test_df[1,1] + confusion_matrix_test_df[1,2]), digits=3)*100`% of them as false.
# 5. **F1 Score:** *2(Precision)(Recall)/(Precision + Recall)*
# * `r 2*(0.753)*(0.714)/(0.753 + 0.714)`
# * The harmonic mean of Precision and Recall, a more balanced view of how good the model is.

```


1. **Accuracy:** *(TP + TN)/Total*
* `r accuracy`
* The model correctly predicts churn or non-churn `r round(accuracy, digits=3)*100`% of the time.
2. **Precision:** *TP/(TP + FP)*
* `r precision`
* Of the values that are predicted true by the model, `r round(precision, digits=3)*100`% of them actually are true.
3. **Recall Rate:** *TP/(TP + FN)*
* `r recall_rate`
* Of the values that actually are true, the model correctly predicts `r round(recall_rate, digits=3)*100`% of them as true.
4. **Specificity:** *TN/(TN + FP)*
* `r specificity`
* Of the values that are actually false, the model correctly predicts, `r round(specificity, digits=3)*100`% of them as false.
5. **F1 Score:** *2(Precision)(Recall)/(Precision + Recall)*
* `r f1_score`
* The harmonic mean of Precision and Recall, a more balanced view of how good the model is.

---

**ROC-AUC: Final Model**

Area under the curve, testing the true positive rate (or recall) against the false positive rate (or specificity)

```{r}
#Get the probability of survival for each passenger from the model
prob_test=predict(final_logit_model, type = "response" )
#Create new column in df of predicted probability of survival 
filtered_joined_churn_df$prob=prob_test
#Determine the ratio of true-positive-rate/false-positive rate as a curve between 0.5 and 1
roc_data <- roc(Churn.Label~prob, data=filtered_joined_churn_df)
#Plot as graph
plot(roc_data)
#Value of area under the curve, prefer 0.8 or higher.
#auc(roc_data2) # area-under-curve 
# unloadPkg("pROC")
```

*Comments*

* This model produces a score of `r auc(roc_data)`
* This is above the threshold of 0.08 for a good fit model

---

**McFadden: Final Model**

```{r, comment=NA}
#calculate McFadden statistics, the pseudo r^2
final_pr2 = pR2(final_logit_model)
#final_pr2[4]

```

The pseudo $r^2$ value for the final model, using the McFadden method of calculation, is `r final_pr2[4]`. This means that `r round(final_pr2[4], digits=3)*100`% of the variance seen in the data is accounted for by this model.

---

**AIC/BIC and Deviance: Final Model**

* The AIC of the final model is: `r final_logit_model$aic` 
* The Residual Deviance of the final model is: `r final_logit_model$deviance` 

---

#### **Adjust Cut Off Value: Final Model**

<br>

**Confusion Matrix: Final Model, 0.26 Cut Off**

```{r results='markup'}

############################################################

#HELP WITH MAKING CUTOFF
# The code for the logistic regression model and the predictions is given below
#log_model_full <- glm(loan_status ~ ., family = "binomial", data = training_set)
#predictions_all_full <- predict(log_model_full, newdata = test_set, type = "response")

# Make a binary predictions-vector using a cut-off of 15%
#pred_cutoff_15 <- ifelse(predictions_all_full > 0.15, 1, 0)

# Construct a confusion matrix
#table(test_set$loan_status, pred_cutoff_15)

############################################################

#0.361
#0.263
#0.265
#0.28 -- 80% recall

#Get actual values of churn/non-churn
predictions_final_model <- final_logit_model$fitted.values
#Get predictions based off of selected cut off value
pred_cutoff <- ifelse(predictions_final_model > 0.26, 1, 0)

############################################################

#Turn from kable into data frame (was having trouble accessing elements as a kable)
confusion_matrix_cutoff_df <- data.frame(table(filtered_joined_churn_df$Churn.Label, pred_cutoff))
#Convert format of df to matrix style
confusion_matrix_cutoff_df <- reshape(confusion_matrix_cutoff_df, idvar = "Var1", timevar = "pred_cutoff", direction = "wide")
#Drop first column and add totals column
confusion_matrix_cutoff_df <- select(confusion_matrix_cutoff_df, c(2,3)) %>%
  mutate(
    Total=Freq.0+Freq.1
  )
#Add total row to capture prediction totals
confusion_matrix_cutoff_df[nrow(confusion_matrix_cutoff_df)+ 1 , ] <- c(sum(confusion_matrix_cutoff_df$Freq.0), sum(confusion_matrix_cutoff_df$Freq.1), sum(confusion_matrix_cutoff_df$Total))
#Add Column at the start of DF to identify the actual values
Index=c("Actual No", "Actual Yes", "Total")
confusion_matrix_cutoff_df <- cbind(Index, confusion_matrix_cutoff_df)
#Rename columns to identify predicted values
colnames(confusion_matrix_cutoff_df) <- c("", "Predicted No", "Predicted Yes", "Total")
#Display Cut Off Confusion Matrix
xkabledply(confusion_matrix_cutoff_df, title="Confusion Matrix for Final Model With a Cut Off Value of 0.26")


########################

#Second value is column

#calculate matric scores with custom cut off
accuracy <- (confusion_matrix_cutoff_df[1,2] + confusion_matrix_cutoff_df[2,3])/confusion_matrix_cutoff_df[3,4]
precision <- confusion_matrix_cutoff_df[2,3]/(confusion_matrix_cutoff_df[2,3] + confusion_matrix_cutoff_df[1,3])
recall_rate <- confusion_matrix_cutoff_df[2,3]/(confusion_matrix_cutoff_df[2,3] + confusion_matrix_cutoff_df[2,2])
specificity <- confusion_matrix_cutoff_df[1,2]/(confusion_matrix_cutoff_df[1,2] + confusion_matrix_cutoff_df[1,3])
f1_score <- 2*(precision)*(recall_rate)/(precision + recall_rate)

# cat('\naccuracy:     ', accuracy, '\n')
# cat('precision:    ', precision, '\n')
# cat('recal:        ', recall_rate, '\n')
# cat('specificity:  ', specificity, '\n')
# cat('f1:           ', f1_score, '\n')

#accuracy
#precision
#recall_rate
#specificity
#f1_score
```

1. **Accuracy:** *(TP + TN)/Total*
* `r accuracy`
* The model correctly predicts churn or non-churn `r round(accuracy, digits=3)*100`% of the time.
2. **Precision:** *TP/(TP + FP)*
* `r precision`
* Of the values that are predicted true by the model, `r round(precision, digits=3)*100`% of them actually are true.
3. **Recall Rate:** *TP/(TP + FN)*
* `r recall_rate`
* Of the values that actually are true, the model correctly predicts `r round(recall_rate, digits=3)*100`% of them as true.
4. **Specificity:** *TN/(TN + FP)*
* `r specificity`
* Of the values that are actually false, the model correctly predicts, `r round(specificity, digits=3)*100`% of them as false.
5. **F1 Score:** *2(Precision)(Recall)/(Precision + Recall)*
* `r f1_score`
* The harmonic mean of Precision and Recall, a more balanced view of how good the model is.

---

**Log Loss, Regular and Brier Score**

```{r, results='markup'}
#Grab only Actual Churn and Predicted Churn
logloss_df <- filtered_joined_churn_df %>%
  select(c(14, 22)) %>%
  mutate(
    corrected_prob=ifelse(Churn.Label=="Yes", prob, 1-prob) #Correct pred based on actual churn
  )
xkabledply(head(logloss_df), title="Example of Model Predictions Corrected for Log Loss Calculation")

logloss_value <- LogLoss(y_pred= final_logit_model$fitted.values, y_true=churn_df$Churn.Value)
#logloss_value
  
#Get the probability of survival for each passenger from the model
prob_test=predict(sat_logit_model, type = "response" )
#Create new column in df of predicted probability of survival 
churn_df$prob=prob_test

#brier_test <- brierscore(data=churn_df, Churn.Value ~ prob)
brier_test <- BrierScore(churn_df$Churn.Value, final_logit_model$fitted.values)
#brier_test

```

When the regular Log Loss function is used, the result is: 

* `r logloss_value`

When the Brier Test Method is used for an unbalanced Data Set, the result is:

* `r brier_test`

---

#### **Final Logistic Model Takeaways**

* Customers with avg Monthly Charge (~$65) and no other mitigating factors have a churn risk of about ~60%
* Long-term contracts biggest factor in keeping customers with the company
* Roughly every ~100 GB of monthly download decays odds-ratio of customer churning by 10%
* The overall avg Monthly Charge of ~$65, has a 3.6-fold growth effect on the odds-ratio of a customer churning
* Reducing the Monthly Charge to $55 decreases the growth effect from 3.6 to 3
* Adjusting the cutoff significantly improves recall rate (+29.1%) while only suffering a minor hit to precision (-7%)
* Brier Test for Log Loss (0.244) shows a model that performs reasonably well 

### 3.1.4 Suggestion Based on Model

Nearly 89% of customers who churned were on month-to-month contracts. It is suggested that the company offer an additional $10 off monthly charges for customers on month-to-month contracts who would be willing to sign a 2-year contract.

For a customer who pays the avg Monthly Charge of ~$65 and who has no other mitigating factors, this would reduce the risk of churning from about ~60% to about ~5%. 

Assuming the ideal scenario were all would-be churners who have month-to-month contracts take the deal and sign a 2-year contract, this theoretically reduces the number of customers who churn in this group by about ~91.7%. 

This would result in the following monetary benefits to the company:

* Using plan with adjusted cut off model saves ~$794K a year by retaining ~65% of would-be churners (91.7% of the 88.6% who have month-to-month contracts of the 79.6% of churners the model catches) 
* Using plan with adjusted cut off model loses an extra ~$76k by offering discounts to customers that wouldn't have churned
* Total value of implementing retainment plan according to model with adjusted cut off: ~$718k a year

---

## 3.2 Random Forest Model

#### EDA feature selection

From the EDA, features we've found out will be useful and will be used in random forest model:

Categorical: 
Partner 
Tech.Support 
Online.Security 
Online.Backup 
Paperless.Billing 
Payment.Method 
Senior Citizen
Contract Type
Satisfaction Score
Dependents

Continuous:
Tenure
Avg. Monthly Charges
Total.Months

#### Data Preparation


**clean categorical feature **

From the previous EDA, we know that there are some categorical features that have 'No' and 'No Internet Service' or 'No Phone Service' as a category, we can make them as 'No' and clean these features.

```{r,results='markup' }
tg <- data.frame(lapply(filtered_joined_churn_df, function(x){ gsub("No internet service", "No", x)}))
tg <- data.frame(lapply(filtered_joined_churn_df, function(x){ gsub("No phone service", "No", x)}))
glimpse(tg)

```


**Standardizing Continuous features**

Continuous festures:
Tenure
Avg. Monthly Charges
Total.Months

```{r,results='markup'}
num_columns <- c("Tenure.Months", "Monthly.Charges", "Total.Extra.Data.Charges")
tg[num_columns] <- sapply(tg[num_columns], as.numeric)

tg_int <- tg[,c("Tenure.Months", "Monthly.Charges", "Total.Extra.Data.Charges")]
tg_int <- data.frame(scale(tg_int))
head(tg_int,5)
```


**Create discrete options of features**
Tenure is highy correlate with target variable, therefore I am trying to create a discrete feature from tenure, where I have made different bins of tenure(which is in months) such as '0-1 year', '2-3 years', '3-4 years' etc. to show how will trnure.months actually affect whether customer choose to churn or not churn.


```{r}
tg <- mutate(tg, tenure_bin = Tenure.Months)

tg$tenure_bin[tg$tenure_bin >=0 & tg$tenure_bin <= 12] <- '0-1 year'
tg$tenure_bin[tg$tenure_bin > 12 & tg$tenure_bin <= 24] <- '1-2 years'
tg$tenure_bin[tg$tenure_bin > 24 & tg$tenure_bin <= 36] <- '2-3 years'
tg$tenure_bin[tg$tenure_bin > 36 & tg$tenure_bin <= 48] <- '3-4 years'
tg$tenure_bin[tg$tenure_bin > 48 & tg$tenure_bin <= 60] <- '4-5 years'
tg$tenure_bin[tg$tenure_bin > 60 & tg$tenure_bin <= 72] <- '5-6 years'

tg$tenure_bin <- as.factor(tg$tenure_bin)

```


After checking the distribution of data in each tenure bin, we found that maximum number of customers have a tenure of either 0-1 years and followed by 5-6 years.

```{r}
options(repr.plot.width =6, repr.plot.height = 3)
ggplot(tg, aes(tenure_bin, fill = tenure_bin)) + geom_bar()
```


**Create dummy**
```{r, results='markup'}
tg_c1 <- tg[,c("Paperless.Billing", "Payment.Method","tenure_bin", "Tech.Support","Online.Backup","Contract","Internet.Service","Partner","Senior.Citizen","Dependents","Churn.Label")]
dummy<- data.frame(sapply(tg_c1,function(x) data.frame(model.matrix(~x-1,data =tg_c1))[,-1]))
tail(dummy, n=5)
```

**Create final dataset**

Creating the final dataset by combining the numeric and dummy data frames.

```{r, results='markup'}
#Combining the data
tg_final <- cbind(tg_int,dummy)
head(tg_final)
tg_final$Churn.Label<- as.factor(tg_final$Churn.Label)
head(tg_final,5)
```

#### Random Forest Model

**Split data**

Splitting data into train and validation

```{r, results='markup'}
#Splitting the data
library(randomForest)
set.seed(123)
indices = sample.split(tg_final$Churn.Label, SplitRatio = 0.7)
train = tg_final[indices,]
validation = tg_final[!(indices),]
xkablesummary(train)
xkablesummary(validation)
```

**Train randomforest model**

```{r, results='markup'}
#Training the RandomForest Model

model.rf <- randomForest(Churn.Label ~ ., data=train, proximity=F,importance = F,ntree=500,mtry=4, do.trace=F, na.action=na.roughfix)
model.rf
```

The OOB error estimate comes to around 20%, so the model has around 80% out of sample accuracy for the training set. Let's check the prediction and accuracy on validation data.

```{r, results='markup'}
testPred <- predict(model.rf, newdata=validation[,-24])
#table(testPred2, validation2$Churn.Label)

confusionMatrix(validation$Churn.Label, testPred)
```
The basic RandomForest model gives an accuracy of 80.3%( almost close enough to the OOB estimate), Sensitivity 85.3% and Specificity 64%.,


**Variable Importance Plot**

Below is the variable importance plot, that shows the most significant attribute in decreasing order by mean decrease in Gini. The Mean decrease Gini measures how pure the nodes are at the end of the tree. Higher the Gini Index, better is the homogeneity.

```{r}
#Checking the variable Importance Plot
varImpPlot(model.rf)
```

  

**Checking the AUC**
```{r}
model.rf.roc <- roc(response = validation$Churn.Label, predictor = as.numeric(testPred))
#print()
#plot(rf.roc, col = "red" , add = TRUE, print.auc.y = 0.85, print.auc = TRUE)
plot(model.rf.roc,col = "red",print.auc.y = 0.85, print.auc = TRUE )
```

#### Brief summary of Model

RandomForest:

Accuracy 80.3%,
Sensitivity 85.3%
Specificity 64%



---

## 3.3 SVM Model

```{r}
str(filtered_joined_churn_df)
```

```{r}
view(filtered_joined_churn_df)
```

```{r}
my_data <- data.frame(filtered_joined_churn_df$Senior.Citizen,filtered_joined_churn_df$Churn.Label,filtered_joined_churn_df$Partner,filtered_joined_churn_df$Dependents,filtered_joined_churn_df$Tenure.Months,filtered_joined_churn_df$Phone.Service,filtered_joined_churn_df$Online.Security,filtered_joined_churn_df$Online.Backup,filtered_joined_churn_df$Tech.Support,filtered_joined_churn_df$Paperless.Billing,filtered_joined_churn_df$Unlimited.Data,filtered_joined_churn_df$Avg.Monthly.Long.Distance.Charges,filtered_joined_churn_df$Avg.Monthly.GB.Download,filtered_joined_churn_df$Monthly.Charges)

str(my_data)
```


```{r}
### Libraries used
library(caret)
library(class)
```

```{r}
## Partition of data
train_sample<-createDataPartition(y = my_data$filtered_joined_churn_df.Churn.Label, p= 0.7, list = FALSE)
train_data<-my_data[train_sample, ]
test_data<-my_data[-train_sample, ]
train_label<-my_data[train_sample,2]
test_label<-my_data[-train_sample,2]
```

```{r}
### Linear SVM
svm_linear <- train(filtered_joined_churn_df.Churn.Label ~., data = train_data, method = "svmLinear", trControl=trainControl(method = "repeatedcv", number = 10, repeats = 3), preProcess = c("center", "scale"), tuneLength = 10)
svm_linear
```

```{r}
### Confusion Matrix
test_pred<-predict(svm_linear,test_data)
confusionMatrix(table(test_pred,test_data$filtered_joined_churn_df.Churn.Label))
```

```{r}
### griding for obtaining best C value
svm_grid<-train(filtered_joined_churn_df.Churn.Label~., data = train_data, method = "svmLinear", trControl=trainControl("cv", number = 10), tuneGrid = expand.grid(C=seq(0,2,length=20)), preProcess = c("center", "scale"))
```

```{r}
# plot model accuracy vs different values of cost
plot(svm_grid)
svm_grid$bestTune
test_predgrid<-predict(svm_grid,test_data)
```

```{r}
# Accuracy after finding best parameter
confusionMatrix(table(test_predgrid,test_data$filtered_joined_churn_df.Churn.Label))
```


```{r results='markup'}
### ROC Curve
library(pROC)
roc_svm_test <-plot(roc(as.numeric(test_data$filtered_joined_churn_df.Churn.Label),as.numeric(test_pred)),main="Comparaison ",col="#1c61b6")
plot(roc_svm_test, add = TRUE,col = "red", print.auc=TRUE, print.auc.x = 0.5, print.auc.y = 0.3)
legend(0.3, 0.2, legend = c("test-svm"), lty = c(1), col = c("blue"))

```

---

## 3.4 Decision Tree Model



---

***Omit NA values***

---

***We have also tried to build the decision tree mode.A decision tree is a type of supervised machine learning used to categorize or make predictions based on how a previous set of questions were answered. The model is a form of supervised learning, meaning that the model is trained and tested on a set of data that contains the desired categorization***

***before building the actual tree , since the data is not so clean, we have tried to clean categorical variables, changing into factors.standardized numerical variables. I have also cleaned the na values***
***we have a used label encoding for categorical variables.***

***We got the insight from our previous eda that tota extra data charges , avg monthly long dist chanrges has null on whether a customer to churn or not so we removed those variables from the model building.***

```{r results='hide'}

loadPkg("rpart")

library(caret)

library(tidyr)

install.packages("dplyr")

library("dplyr")

filtered_joined_churn_df %>% na.omit(filtered_joined_churn_df)





filtered_joined_churn_df[complete.cases(filtered_joined_churn_df), ]

filtered_joined_churn_df %>% drop_na()





```

```{r results='markup'}

control1 <- rfeControl(functions = rpart, # random forest
                      method = "repeatedcv", # repeated cv
                      repeats = 5, # number of repeats
                      number = 10) # number of folds




```

**Label encoding the categorical variables**

```{r results='markup'}



head(filtered_joined_churn_df)




library(superml)




label <- LabelEncoder$new()


filtered_joined_churn_df$Internet.Service <- label$fit_transform(filtered_joined_churn_df$Internet.Service)



filtered_joined_churn_df$Contract <- label$fit_transform(filtered_joined_churn_df$Contract)



filtered_joined_churn_df$Payment.Method <- label$fit_transform(filtered_joined_churn_df$Payment.Method)









```



```{r results='markup'}


filtered_joined_churn_df$Churn.Label <- ifelse(filtered_joined_churn_df$Churn.Label == "Yes",1,0)

filtered_joined_churn_df$Dependents  <- ifelse(filtered_joined_churn_df$Dependents  == "Yes",1,0)

filtered_joined_churn_df$Senior.Citizen <- ifelse(filtered_joined_churn_df$Senior.Citizen== "Yes",1,0)

filtered_joined_churn_df$Partner <- ifelse(filtered_joined_churn_df$Partner== "Yes",1,0)

#filtered_joined_churn_df$Tenure.Months <- ifelse(filtered_joined_churn_df$Tenure.Months == "Yes",1,0)

filtered_joined_churn_df$Phone.Service <- ifelse(filtered_joined_churn_df$Phone.Service == "Yes",1,0)

filtered_joined_churn_df$Online.Security <- ifelse(filtered_joined_churn_df$Online.Security == "Yes",1,0)

filtered_joined_churn_df$Online.Backup <- ifelse(filtered_joined_churn_df$Online.Backup == "Yes",1,0)

filtered_joined_churn_df$Tech.Support <- ifelse(filtered_joined_churn_df$Tech.Support == "Yes",1,0)

filtered_joined_churn_df$Paperless.Billing <- ifelse(filtered_joined_churn_df$Paperless.Billing == "Yes",1,0)

filtered_joined_churn_df$Unlimited.Data <- ifelse(filtered_joined_churn_df$Unlimited.Data == "Yes",1,0)





head(filtered_joined_churn_df)




myvars <- c("Churn.Label","Senior.Citizen", "Partner", "Dependents","Tenure.Months","Phone.Service","Online.Security","Online.Backup","Tech.Support","Paperless.Billing","Unlimited.Data","Avg.Monthly.Long.Distance.Charges","Avg.Monthly.GB.Download","Monthly.Charges","Tenure.Months")



n <- filtered_joined_churn_df[myvars]



my <- c("Senior.Citizen", "Partner", "Dependents","Tenure.Months","Phone.Service","Online.Security")

u = filtered_joined_churn_df[my]




a <- n[myvars]

typeof(a)

nrow(a)

head(a)



myvars <- c("Senior.Citizen","Churn.Label", "Partner", "Dependents","Tenure.Months","Phone.Service","Online.Security","Online.Backup","Tech.Support","Paperless.Billing","Unlimited.Data","Avg.Monthly.Long.Distance.Charges","Avg.Monthly.GB.Download","Monthly.Charges","Tenure.Months")





n$Churn.Label <- as.factor(n$Churn.Label)
                              




head(n$Churn.Label)

typeof(n$Churn.Label)


b = as.factor(n["Churn.Label"])








```

**Features of variable importance**

```{r results='markup'}

fit_dt = rpart(Churn.Label~., data=filtered_joined_churn_df, method="class", control = list(maxdepth = 4))

varImp(fit_dt)

```

**Decision Tree with Gini**


```{r results='markup'}

myvars <- c("Senior.Citizen","Churn.Label", "Partner", "Dependents","Tenure.Months","Phone.Service","Online.Security","Online.Backup","Tech.Support","Paperless.Billing","Unlimited.Data","Avg.Monthly.GB.Download","Tenure.Months","Internet.Service","Contract")


model_df <- filtered_joined_churn_df[myvars]

```

**We have removed Total.Extra.Data.Charges and  Avg.Monthly.Long.Distance.Charges , as it is not significant from the test we performed above**


```{r results='markup'}
set.seed(56)
library(caret)
split_train_test <- createDataPartition(model_df$Churn.Label,p=0.7,list=FALSE)
dtrain<- model_df[split_train_test,]
dtest<-  model_df[-split_train_test,]

# Remove Total Charges from the training dataset



```

---

**Train-Test data**

---

```{r results='markup'}

head(dtrain)

head(dtest)




```



```{r results='markup'}


loadPkg("rpart")

library(caret)

library(tidyr)

install.packages("dplyr")

library("dplyr")

library(superml)

loadPkg("caret")

loadPkg("rpart.plot")

loadPkg("rattle")

tr_fit <- rpart(Churn.Label ~., data = dtrain, method="class",na.action = na.exclude)
rpart.plot(tr_fit)

summary(tr_fit)

```

---

**From this decision tree, we can interpret the following:**

**The contract variable is the most important. Customers with month-to-month contracts are more likely to churn.Customers who has a dependents (value=1) has the higher chance to churn.** 
**Customers with DSL internet service are less likely to churn.**
**Customers who have stayed longer than 54 months are less likely to churn. Now let’s assess the prediction accuracy of the decision tree model by investigating how well it predicts churn in the test subset. We will begin with the confustion matrix, which is a useful display of classification accuracy. It displays the following information:**

---

**true positives (TP): These are cases in which we predicted yes (they churned), and they did churn. true negatives (TN): We predicted no, and they didn’t churn. false positives (FP): We predicted yes, but they didn’t actually churn. (Also known as a “Type I error.”) false negatives (FN): We predicted no, but they actually churned. (Also known as a “Type II error.”) Let’s examine the confusion matrix for our decision tree model.**


---

```{r results='markup'}

tr_prob1 <- predict(tr_fit, dtest)
tr_pred1 <- ifelse(tr_prob1[,2] > 0.5,"Yes","No")
table(Predicted = tr_pred1, Actual = dtest$Churn.Label)


```

---

***From this confusion matrix, we can see that the model performs well at predicting non-churning customers (1354 correct vs. 208 incorrect) and  predicting churning customers 341 correct vs. 209 incorrect).***

---

```{r results='markup'}


tr_prob2 <- predict(tr_fit, dtrain)
tr_pred2 <- ifelse(tr_prob2[,2] > 0.5,"Yes","No")
tr_tab1 <- table(Predicted = tr_pred2, Actual = dtrain$Churn.Label)
tr_tab2 <- table(Predicted = tr_pred1, Actual = dtest$Churn.Label)

```

```{r results='markup'}


# # Train
# confusionMatrix(
#   as.factor(tr_pred2),
#   as.factor(dtrain$Churn.Label),na.action = na.pass,
#   positive = "Yes" 
# )
# 
# #Test
# confusionMatrix(
# as.factor(tr_pred1),
#   as.factor(dtest$Churn.Label),
#   positive = "Yes" 
# )

tr_acc <- sum(diag(tr_tab2))/sum(tr_tab2)
tr_acc

```

---

**The overall accuracy of the model is `r round(tr_acc*100, digits=2)`%**


---

```{r results='markup'}
library(pROC)
ROC_rf <- roc(dtest$Churn.Label, tr_prob1[,2])


ROC_rf_auc <- auc(ROC_rf)

plot(ROC_rf, col = "green", main = "ROC Curve for the decision tree with gini")


paste("Accuracy decision tree with gini: ", mean(dtest$Churn.Labe == round(tr_prob1[,2], digits = 0)))

paste("Area under curve for decision tree with gini is: ", ROC_rf_auc)


```

---

**This is the auc roc curve for the decision tree with gini parameter**

***The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.***

---

**The overall accuracy of the decision tree with gini model is `r round(tr_acc*100, digits=2)`%**
**The AUC of the decision tree with gini  model is `r round(ROC_rf_auc, digits=2)`**

---


**Decision Tree with Entropy- rpart**


```{r results='markup'}

myvars <- c("Senior.Citizen","Churn.Label", "Partner", "Dependents","Tenure.Months","Phone.Service","Online.Security","Online.Backup","Tech.Support","Paperless.Billing","Unlimited.Data","Avg.Monthly.GB.Download","Tenure.Months","Internet.Service","Contract")


model_df <- filtered_joined_churn_df[myvars]

```

**We have removed Total.Extra.Data.Charges and  Avg.Monthly.Long.Distance.Charges , as it is not significant from the test we performed above**


```{r results='markup'}
set.seed(56)
library(caret)
split_train_test <- createDataPartition(model_df$Churn.Label,p=0.7,list=FALSE)
dtrain<- model_df[split_train_test,]
dtest<-  model_df[-split_train_test,]

# Remove Total Charges from the training dataset



```

---

**Train-Test data**

---

```{r results='markup'}

head(dtrain)

head(dtest)




```

```{r results='markup'}


loadPkg("rpart")

library(caret)

library(tidyr)

install.packages("dplyr")

library("dplyr")

library(superml)

loadPkg("caret")

loadPkg("rpart.plot")

loadPkg("rattle")

tr_fit <- rpart(Churn.Label ~., data = dtrain, method="class",na.action = na.exclude,parms=list(split='entropy'))
rpart.plot(tr_fit)

summary(tr_fit)

```

---

**The overall accuracy of the tree model with entropy is `r round(tr_acc*100, digits=2)`%**

**From this decision tree, we can interpret the following:**

**The contract variable is the most important. Customers with month-to-month contracts are more likely to churn.Customers who has a dependents (value=1) has the higher chance to churn.** 
**Customers with DSL internet service are less likely to churn.**
**Customers who have stayed longer than 54 months are less likely to churn. Now let’s assess the prediction accuracy of the decision tree model by investigating how well it predicts churn in the test subset. We will begin with the confustion matrix, which is a useful display of classification accuracy. It displays the following information:**

---

**true positives (TP): These are cases in which we predicted yes (they churned), and they did churn. true negatives (TN): We predicted no, and they didn’t churn. false positives (FP): We predicted yes, but they didn’t actually churn. (Also known as a “Type I error.”) false negatives (FN): We predicted no, but they actually churned. (Also known as a “Type II error.”) Let’s examine the confusion matrix for our decision tree model.**


---

```{r results='markup'}

tr_prob1 <- predict(tr_fit, dtest)
tr_pred1 <- ifelse(tr_prob1[,2] > 0.5,"Yes","No")
table(Predicted = tr_pred1, Actual = dtest$Churn.Label)


```

```{r results='markup'}


tr_prob2 <- predict(tr_fit, dtrain)
tr_pred2 <- ifelse(tr_prob2[,2] > 0.5,"Yes","No")
tr_tab1 <- table(Predicted = tr_pred2, Actual = dtrain$Churn.Label)
tr_tab2 <- table(Predicted = tr_pred1, Actual = dtest$Churn.Label)

```

```{r results='markup'}


# # Train
# confusionMatrix(
#   as.factor(tr_pred2),
#   as.factor(dtrain$Churn.Label),na.action = na.pass,
#   positive = "Yes" 
# )
# 
# #Test
# confusionMatrix(
# as.factor(tr_pred1),
#   as.factor(dtest$Churn.Label),
#   positive = "Yes" 
# )

tr_acc <- sum(diag(tr_tab2))/sum(tr_tab2)
tr_acc

```

```{r results='markup'}
library(pROC)
ROC_rf <- roc(dtest$Churn.Label, tr_prob1[,2])


ROC_rf_auc <- auc(ROC_rf)

plot(ROC_rf, col = "green", main = "ROC Curve for the decision tree with entropy")


paste("Accuracy decision tree with entropy: ", mean(dtest$Churn.Labe == round(tr_prob1[,2], digits = 0)))

paste("Area under curve for decision tree with entropy is: ", ROC_rf_auc)


```

**The overall accuracy of the decision tree with entropy model is `r round(tr_acc*100, digits=2)`%**
**The AUC of the decision tree with entropy model is `r round(ROC_rf_auc, digits=2)`**

---

***We have build the decision tree with entropy parameter but surprisingly it gave the same result as given by gini decision tree model.***

---

**Prune trees**


**Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances.**

**we have used the control paramteres for the tree for prunning as :**

**cp = 0.02,maxcompete = 2, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 10.**

**cp:The complexity parameter (cp) in rpart is the minimum improvement in the model needed at each node. It’s based on the cost complexity of the model maxsurrogate :number of surrogate splits to evaluate.**


**maxdepth : Set the maximum depth of any node of the final tree,xval:  number of cross-validations**


---

```{r results='markup'}	



tr_fit <- rpart(Churn.Label ~., data = dtrain, method="class",na.action = na.exclude,control = rpart.control(cp = 0.02,maxcompete = 2, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 10 ))

rpart.plot(tr_fit)



summary(tr_fit)

```

---

**From this decision tree, we can interpret the following:**

**The contract variable is the most important. Customers with month-to-month contracts are more likely to churn.Customers who has a dependents (value=1) has the higher chance to churn.** 
**Customers with DSL internet service are less likely to churn.**
**Customers who have stayed longer than 16 months(previously 54) are less likely to churn. Now let’s assess the prediction accuracy of the decision tree model by investigating how well it predicts churn in the test subset. We will begin with the confustion matrix, which is a useful display of classification accuracy. It displays the following information:**

---

**true positives (TP): These are cases in which we predicted yes (they churned), and they did churn. true negatives (TN): We predicted no, and they didn’t churn. false positives (FP): We predicted yes, but they didn’t actually churn. (Also known as a “Type I error.”) false negatives (FN): We predicted no, but they actually churned. (Also known as a “Type II error.”) Let’s examine the confusion matrix for our decision tree model.From this confusion matrix, we can see that the model performs well at predicting non-churning customers (1426 correct vs. 272 incorrect) and  predicting churning customers 277  correct vs. 137 incorrect).**


---

```{r results='markup'}

tr_prob1 <- predict(tr_fit, dtest)
tr_pred1 <- ifelse(tr_prob1[,2] > 0.5,"Yes","No")
table(Predicted = tr_pred1, Actual = dtest$Churn.Label)


```

```{r results='markup'}


tr_prob2 <- predict(tr_fit, dtrain)
tr_pred2 <- ifelse(tr_prob2[,2] > 0.5,"Yes","No")
tr_tab1 <- table(Predicted = tr_pred2, Actual = dtrain$Churn.Label)
tr_tab2 <- table(Predicted = tr_pred1, Actual = dtest$Churn.Label)

```

```{r results='markup'}


# # Train
# confusionMatrix(
#   as.factor(tr_pred2),
#   as.factor(dtrain$Churn.Label),na.action = na.pass,
#   positive = "Yes" 
# )
# 
# #Test
# confusionMatrix(
# as.factor(tr_pred1),
#   as.factor(dtest$Churn.Label),
#   positive = "Yes" 
# )

tr_acc <- sum(diag(tr_tab2))/sum(tr_tab2)
tr_acc

```

```{r results='markup'}
library(pROC)
ROC_rf <- roc(dtest$Churn.Label, tr_prob1[,2])


ROC_rf_auc <- auc(ROC_rf)

plot(ROC_rf, col = "green", main = "ROC Curve for the prune tree")


paste("Accuracy : ", mean(dtest$Churn.Labe == round(tr_prob1[,2], digits = 0)))

paste("Area under curve for prune tree model is: ", ROC_rf_auc)


```


---

**The overall accuracy of the prune tree model is `r round(tr_acc*100, digits=2)`%**
**The AUC of the prune tree model is `r round(ROC_rf_auc, digits=2)`**

---

**Prune model is best than other models in terms of accuracy, efficiency and roc value**

---




# 4. Model Comparisons

## 4.1 Accuarcy of Models
 
Accuracy of each model:
Logistic Regression :73.5%
Random Forest Model: 80.3%
SVM: 80.9%
Decision Tree: 80.63%



The Prune Tree Model had the best combined accuracy and AUC




*More to add in summary paper*

=======

---

## 4.3 Final Model Selection

After comparing the accuracy, sensitivity, specificity, and AUC value, the prune tree has the best performence in all cases. Therefore the prune tree is selected as the final model.


The overall accuracy of the decision tree with entropy model is 80.26% The AUC of the decision tree with entropy model is 0.82

The overall accuracy of the prune tree model is 80.63% The AUC of the prune tree model is 0.82.

Sensity : 50.45%
Specificity : 91.23%

Prune gives higher accuacy with lower levels than decision tree with entropy or gini parametre.

As this is a binary classification, we have used auc score for comparing the model as a parametre .
Random forest has close accuracy to the prune tree , but lower auc score than prune tree,also the decision tree model gives high importance to a particular set of features. But the random forest chooses features randomly during the training process. Therefore, it does not depend highly on any specific set of features,making to opt for prune tree over random forest.



Decision tree has 9 levels where prune tree has only 6 levels. Prune tree gives more clear picture of feature 'Tenure months' and 'Average monthly gb downloaded'

*More to add in summary paper*

=======

---

# 5. Answering SMART Questions

We have two questions:

**Customers with what behaviors and conditions would likely leave the platform?**

- From the prune tree, Customers with month-to-month contracts are more likely to churn. Customers who has a dependents (value=1) has the higher chance to churn. Customers with DSL internet service are less likely to churn. Customers who have stayed longer than 16 months(previously 54) are less likely to churn.

**What services are important to deliver to a customer to keep them with the company?**

- According to the model analysis, we found a couple of important features that would affect whether customers decide to churn or not churn. For example,vfeatures from the random forest model would make impact on whether customers decide to churn or not churn include total charges, tenure months, monthly charges, internet service with fiber optic, dependents, payment method with an electronic check, contract two years, contract one year, tech support, and paperless billing. Since total charges, tenure months, and monthly charges rank the top 3 in importance, therefore a reasonable price plan will be important to keep customer with plaforms.


- Another example, as explored in the Logistic Model, would be developing special deals to offer customers in order to get them to agree to a 1-year or 2-year contract, as being in a long-term contract significantly reduced the risk of churning. Several of the models found this feature to be an important one.

