---
title: "Final_project6101"
author: "Jane"
date: "4/16/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load Package
```{r}
library(tidyverse)
library(knitr)
library(kableExtra)
library(xtable)
library(Hmisc)
library(MASS)
library(car)
library(e1071)
library(caret)
library(cowplot)
library(caTools)
library(pROC)
library(ggcorrplot)
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
 
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times

```

```{r}
loadPkg("ggplot2")
```

# Featue Selection

From the EDA, features we've found out will be useful:

Categorical: 
Partner 
Tech.Support 
Online.Security 
Online.Backup 
Paperless.Billing 
Payment.Method 
Senior Citizen
Contract Type
Satisfaction Score
Dependents

Continuous:
Tenure
Avg. Monthly Charges

??? do we need have plots here as brief explanations as well as the rationale of the EDA and models??


# Model Selection

??? do we need to use model selection technique that we learned from class( like exhaustive test.etc) to determin model?
??? but  since the target variable has binary classifiers so Logistic reg

## 1. Logistic Reg

load data:
```{r}
tg <- data.frame(read.csv("~/GitHub/group7_project1/Data Files/Telco_customer_churn.csv")) 
head(tg)
xkablesummary(tg)

```

Categorical Variable

```{r}
tg$Churn.Value<- factor(tg$Churn.Value)
tg$Paperless.Billing <- factor(tg$Paperless.Billing)
tg$Payment.Method <- factor(tg$Payment.Method)
tg$Tech.Support <- factor(tg$Tech.Support)
tg$Online.Backup <- factor(tg$Online.Backup)
tg$Contract <- factor(tg$Contract)
tg$Internet.Service <- factor(tg$Internet.Service)
tg$Partner <- factor(tg$Partner)
tg$Dependents <- factor(tg$Dependents)
tg$Senior.Citizen <- factor(tg$Senior.Citizen)
```

```{r}

tgLogit <- glm(Churn.Value ~ Paperless.Billing+Payment.Method+Tech.Support+Online.Backup+Contract+Internet.Service+Partner+Senior.Citizen+Dependents , data = tg, family = "binomial")
```

```{r}
xkabledply(tgLogit, title = paste("Logistic Regression :", format(formula(tgLogit)) ))
```


```{r}
summary(tgLogit)
```

## 2. Random forest

### clean categorical feature 
```{r}
glimpse(tg)

```


```{r}
tg <- data.frame(lapply(tg, function(x){ gsub("No internet service", "No", x)}))
tg <- data.frame(lapply(tg, function(x){ gsub("No phone service", "No", x)}))
#str(tg)

glimpse(tg)

```

### Standardising Continuous features
```{r}
# null value in total charge
tg$Total.Charges[is.na(tg$Total.Charges)] <- mean(tg$Total.Charges, na.rm = T) 
```


```{r}
num_columns <- c("Tenure.Months", "Monthly.Charges", "Total.Charges")
tg[num_columns] <- sapply(tg[num_columns], as.numeric)

tg_int <- tg[,c("Tenure.Months", "Monthly.Charges", "Total.Charges")]
tg_int <- data.frame(scale(tg_int))
```

###  create discrete options of features
```{r}
tg <- mutate(tg, tenure_bin = Tenure.Months)

tg$tenure_bin[tg$tenure_bin >=0 & tg$tenure_bin <= 12] <- '0-1 year'
tg$tenure_bin[tg$tenure_bin > 12 & tg$tenure_bin <= 24] <- '1-2 years'
tg$tenure_bin[tg$tenure_bin > 24 & tg$tenure_bin <= 36] <- '2-3 years'
tg$tenure_bin[tg$tenure_bin > 36 & tg$tenure_bin <= 48] <- '3-4 years'
tg$tenure_bin[tg$tenure_bin > 48 & tg$tenure_bin <= 60] <- '4-5 years'
tg$tenure_bin[tg$tenure_bin > 60 & tg$tenure_bin <= 72] <- '5-6 years'
```

```{r}
tg$tenure_bin <- as.factor(tg$tenure_bin)
```

```{r}
options(repr.plot.width =6, repr.plot.height = 3)
ggplot(tg, aes(tenure_bin, fill = tenure_bin)) + geom_bar()
```

### creat dummy

```{r}
tg_c1 <- tg[,c("Paperless.Billing", "Payment.Method","tenure_bin", "Tech.Support","Online.Backup","Contract","Internet.Service","Partner","Senior.Citizen","Dependents","Churn.Label")]

```



```{r}
#creat dummy
dummy<- data.frame(sapply(tg_c1,function(x) data.frame(model.matrix(~x-1,data =tg_c1))[,-1]))

tail(dummy, n=20)

```
```{r}
#Combining the data
tg_final <- cbind(tg_int,dummy)
head(tg_final)
tg_final$Churn.Label<- as.factor(tg_final$Churn.Label)
```


### **creat final data set
```{r}
tg_int2 <- tg_int[,c("Monthly.Charges", "Total.Charges")]
```


```{r}
#Combining the data
tg_final <- cbind(tg_int2,dummy)
head(tg_final)
tg_final$Churn.Label<- as.factor(tg_final$Churn.Label)
```

#### Not using discerete Tenure.Month model
```{r}
head(tg,5)
```

```{r}
#final_data
tg_c <- tg[,c("Paperless.Billing", "Payment.Method","Tenure.Months","Tech.Support","Online.Backup","Contract","Internet.Service","Partner","Senior.Citizen","Dependents","Churn.Label")]
dummy0<- data.frame(sapply(tg_c,function(x) data.frame(model.matrix(~x-1,data =tg_c1))[,-1]))

#tail(dummy, n=20)
#Combining the data
tg_final0 <- cbind(tg_int,dummy0)
#head(tg_final)
tg_final0$Churn.Label<- as.factor(tg_final0$Churn.Label)

```


```{r}
#Splitting the data
library(randomForest)
set.seed(123)
indices = sample.split(tg_final$Churn.Label, SplitRatio = 0.7)
train = tg_final[indices,]
validation = tg_final[!(indices),]
```

The OOB error estimate comes to around 19.4%, so the model has around 80% out of sample accuracy for the training set. Let's check the prediction and accuracy on validation data.

```{r}
#Training the RandomForest Model

model <- randomForest(Churn.Label ~ ., data=train, proximity=F,importance = F,ntree=500,mtry=4, do.trace=F, na.action=na.roughfix)
model
```
```{r}
testPred <- predict(model, newdata=validation[,-24])
#table(testPred2, validation2$Churn.Label)

confusionMatrix(validation$Churn.Label, testPred)
```
```{r}
#Checking the variable Importance Plot
varImpPlot(model)
```




#### AUC
```{r}
model.roc <- roc(response = validation$Churn.Label, predictor = as.numeric(testPred))
#print()
#plot(rf.roc, col = "red" , add = TRUE, print.auc.y = 0.85, print.auc = TRUE)
plot(model.roc,col = "red",print.auc.y = 0.85, print.auc = TRUE )
```



#### overfitting


### splitting train test data

```{r}
#Splitting the data
library(randomForest)
set.seed(123)
indices = sample.split(tg_final$Churn.Label, SplitRatio = 0.7)
train = tg_final[indices,]
validation = tg_final[!(indices),]
```

```{r}
#Training the RandomForest Model

model.rf <- randomForest(Churn.Label ~ ., data=train, proximity=F,importance = F,ntree=500,mtry=4, do.trace=F, na.action=na.roughfix)
model.rf
```



```{r}
testPred <- predict(model.rf, newdata=validation[,-24])
#table(testPred2, validation2$Churn.Label)

confusionMatrix(validation$Churn.Label, testPred)
```

```{r}
#Checking the variable Importance Plot
varImpPlot(model.rf)
```




#### AUC
```{r}
model.rf.roc <- roc(response = validation$Churn.Label, predictor = as.numeric(testPred))
#print()
#plot(rf.roc, col = "red" , add = TRUE, print.auc.y = 0.85, print.auc = TRUE)
plot(model.rf.roc,col = "red",print.auc.y = 0.85, print.auc = TRUE )
```
#

```{r}
tg_c2 <- tg[,c("Payment.Method", "Tech.Support","Contract","Internet.Service","Dependents","Churn.Label")]

```

```{r}
#creat dummy
dummy2<- data.frame(sapply(tg_c2,function(x) data.frame(model.matrix(~x-1,data =tg_c2))[,-1]))

#ail(dummy2, n=20)

```

### creat final data set

```{r}
#Combining the data
tg_final2 <- cbind(tg_int,dummy2)
head(tg_final2)
tg_final2$Churn.Label<- as.factor(tg_final2$Churn.Label)
```

```{r}
#Splitting the data
set.seed(123)
indices2 = sample.split(tg_final2$Churn.Label, SplitRatio = 0.7)
train2 = tg_final2[indices2,]
validation2 = tg_final2[!(indices2),]
```

```{r}
rf<- randomForest(Churn.Label ~ ., data=train2, proximity=F,importance = F,ntree=500,mtry=4, do.trace=F, na.action=na.roughfix)
print(rf)
```
# accuracy = 80%
# OOB and accuracy 
```{r}
testPred2 <- predict(rf, newdata=validation2[,-24])
#table(testPred2, validation2$Churn.Label)

confusionMatrix(validation2$Churn.Label, testPred2)
```
```{r}
#Checking the variable Importance Plot
varImpPlot(rf)
```



```{r}
rf.roc <- roc(response = validation2$Churn.Label, predictor = as.numeric(testPred2))
#print()
#plot(rf.roc, col = "red" , add = TRUE, print.auc.y = 0.85, print.auc = TRUE)
plot(rf.roc,col = "red",print.auc.y = 0.85, print.auc = TRUE )
```


### heatmap

```{r}
cormat <- round(cor(as.numeric(tg_final)),2)
head(cormat)
```

 use filtered_joined_churn_df
 ..
 
## Model Evaluation and comparsion
